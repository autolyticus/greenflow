<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>GreenFlow documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="requirements.html"><strong aria-hidden="true">2.</strong> Requirements</a></li><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">3.</strong> System Overview</a></li><li class="chapter-item expanded "><a href="workflow.html"><strong aria-hidden="true">4.</strong> Workflow</a></li><li class="chapter-item expanded "><a href="prometheus-setup.html"><strong aria-hidden="true">5.</strong> Prometheus Setup</a></li><li class="chapter-item expanded "><a href="data-analysis.html"><strong aria-hidden="true">6.</strong> Data Analysis</a></li><li class="chapter-item expanded affix "><a href="project-info.html">Project Info</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">GreenFlow documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>GreenFlow is a comprehensive toolkit designed to facilitate experiments measuring energy consumption in distributed cloud settings. Typically run in data center environments, it leverages a cloud-native workflow using Kubernetes, enabling precise measurement and attribution of power consumption to specific processes and nodes through the Scaphandre toolkit.</p>
<h2 id="system-overview"><a class="header" href="#system-overview">System Overview</a></h2>
<p>GreenFlow consists of several key components working together to provide a robust experimental framework:</p>
<ol>
<li>
<p><strong>Core Python Package</strong>: Contains the main logic for orchestration, state handling, and experiment storage.</p>
</li>
<li>
<p><strong>Entrypoint</strong>: The <code>entrypoint.py</code> file serves as the main entry point for running experiments. It handles command-line interactions and imports necessary modules.</p>
</li>
<li>
<p><strong>Configuration Management</strong>: Utilizes the Gin library for dependency injection and configuration management.</p>
</li>
<li>
<p><strong>Prometheus Integration</strong>: Used for monitoring and data collection, with support for remote write capability. See <a href="./prometheus-setup.html">Prometheus Setup</a> for more information.</p>
</li>
</ol>
<p>For a complete overview of the system architecture, see <a href="./overview.html">System Overview</a>.</p>
<h2 id="energy-measurement-with-scaphandre"><a class="header" href="#energy-measurement-with-scaphandre">Energy Measurement with Scaphandre</a></h2>
<p>GreenFlow leverages Scaphandre for detailed energy breakdowns. Key points to note:</p>
<ul>
<li>Relies on Intel RAPL (Running Average Power Limit) technology.</li>
<li>Requires a recent Linux kernel with PowerCap interface enabled.</li>
<li>Needs bare-metal access to the cluster for CPU register access.</li>
<li>Measures energy consumption for CPU and RAM, but not peripherals, storage, or cooling systems.</li>
</ul>
<p>For more on energy measurement, see the <a href="https://hubblo-org.github.io/scaphandre/book/compatibility.html">Scaphandre documentation</a></p>
<h2 id="workflow-overview"><a class="header" href="#workflow-overview">Workflow Overview</a></h2>
<p>GreenFlow's workflow, primarily designed for the Grid'5000 testbed, involves:</p>
<ol>
<li><strong>Provisioning</strong>: Making reservations using OAR Job system to obtain bare-metal hardware resources</li>
<li><strong>Deployment of Kubernetes</strong>: GreenFlow makes use of the K3S distribution</li>
<li><strong>Infrastructure Deployment</strong>: Setting up Scaphandre, Prometheus, and other necessary services.</li>
<li><strong>Experiment Execution</strong>: Running one or more experiments within a deployment.</li>
<li><strong>Data Collection and Analysis</strong>: Collecting and analyzing experiment results.</li>
<li><strong>Tear Down</strong>: All the resources are destroyed and released.</li>
</ol>
<p>For a detailed explanation of the workflow, refer to <a href="./workflow.html">Workflow Overview</a>.</p>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<p>To begin using GreenFlow:</p>
<ol>
<li>Review the <a href="./requirements.html">Requirements</a> for running GreenFlow.</li>
<li>Learn how to design and run experiments in <a href="./workflow.html">Running Experiments</a>.</li>
<li>Understand how to analyze your results in <a href="./data-analysis.html">Data Collection and Analysis</a>.</li>
</ol>
<p>By following this documentation, you'll be able to run reproducible experiments to measure and analyze energy consumption in distributed systems using GreenFlow.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="requirements"><a class="header" href="#requirements">Requirements</a></h1>
<h2 id="local-project-setup"><a class="header" href="#local-project-setup">Local Project Setup</a></h2>
<p>Setting up the GreenFlow project locally is streamlined thanks to the use of Nix for dependency management and environment configuration. This approach ensures consistency across different development environments and simplifies the setup process.</p>
<h3 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h3>
<p>Before you begin, ensure you have the following installed on your system:</p>
<ol>
<li><strong>Nix</strong>: We use the Determinate Systems Nix installer for a consistent setup.</li>
<li><strong>direnv</strong> (optional, recommended): This tool automatically loads and unloads environment variables based on the current directory.</li>
<li><strong>Docker</strong>: Required for running Docker Compose to set up Tailscale, Caddy, Prometheus, Victoria Metrics, and Grafana.</li>
<li><strong>Docker Compose</strong>: Used to manage multi-container Docker applications.</li>
</ol>
<h3 id="installation-steps"><a class="header" href="#installation-steps">Installation Steps</a></h3>
<ol>
<li>
<p><strong>Install Nix</strong> (if not already installed):</p>
<p>Use the Determinate Systems Nix installer by running the following command in your terminal:</p>
<pre><code class="language-bash">curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install
</code></pre>
</li>
<li>
<p><strong>Install <code>direnv</code></strong>:</p>
<p>Installation methods may vary depending on your operating system. For most Unix-like systems, you can use your package manager. For example:</p>
<ul>
<li>On macOS with Homebrew:
<pre><code class="language-bash">brew install direnv
</code></pre>
</li>
<li>On Ubuntu or Debian:
<pre><code class="language-bash">sudo apt-get install direnv
</code></pre>
</li>
</ul>
<p>For other systems, refer to the <a href="https://direnv.net/docs/installation.html">direnv installation guide</a>.</p>
</li>
<li>
<p><strong>Install Docker</strong>:</p>
<p>Follow the <a href="https://docs.docker.com/get-docker/">official Docker installation guide</a> for your operating system.</p>
</li>
<li>
<p><strong>Install Docker Compose</strong>:</p>
<p>Follow the <a href="https://docs.docker.com/compose/install/">official Docker Compose installation guide</a> for your operating system.</p>
</li>
<li>
<p><strong>Clone the GreenFlow repository</strong>:</p>
<pre><code class="language-bash">git clone https://github.com/autolyticus/greenflow.git
cd greenflow
</code></pre>
</li>
<li>
<p><strong>Set up direnv</strong>:</p>
<pre><code class="language-bash">direnv allow
</code></pre>
<p>This command should automatically download and set up all required dependencies, including Ansible, Python, and any other project-specific tools.</p>
</li>
</ol>
<h3 id="verifying-the-setup"><a class="header" href="#verifying-the-setup">Verifying the Setup</a></h3>
<p>After running <code>nix develop</code>, your environment should be ready for GreenFlow development. You can verify the setup by running:</p>
<pre><code class="language-bash">python --version
ansible --version
</code></pre>
<p>These commands should display the versions of Python and Ansible specified in the project's Nix configuration.</p>
<h3 id="automatic-environment-activation"><a class="header" href="#automatic-environment-activation">Automatic Environment Activation</a></h3>
<p>With <code>direnv</code> properly set up, the Nix environment will automatically activate when you enter the project directory and deactivate when you leave it. This ensures that you're always using the correct versions of tools and dependencies for the GreenFlow project.</p>
<h3 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h3>
<p>If you encounter any issues during the setup process:</p>
<ol>
<li>Ensure that Nix and direnv are correctly installed and configured.</li>
<li>Check that your <code>.envrc</code> file is properly set up and allowed by direnv.</li>
<li>If changes are made to the Nix configuration, you may need to run <code>nix develop</code> again to update the environment.</li>
</ol>
<h3 id="flakenix-configuration"><a class="header" href="#flakenix-configuration">flake.nix Configuration</a></h3>
<p>The <code>flake.nix</code> file in the project root provides a clear definition of the dependencies and environment setup for the GreenFlow project. It includes:</p>
<ul>
<li><strong>Nix Packages</strong>: A list of packages required for the project, such as <code>micromamba</code>, <code>kubectl</code>, <code>kubernetes-helm</code>, and more.</li>
<li><strong>Scripts</strong>: Scripts for resetting the environment and setting up dependencies.</li>
<li><strong>Environment Variables</strong>: Configuration for environment variables and shell hooks.</li>
</ul>
<p>This configuration ensures that all necessary tools and dependencies are available and correctly set up for the GreenFlow project.</p>
<h3 id="additional-tools"><a class="header" href="#additional-tools">Additional Tools</a></h3>
<p>If you do not have networking access to expose your local experiment machine to the Kubernetes cluster, you will need to set up Tailscale, Caddy, Victoria Metrics, and Grafana using Docker Compose from <code>$PROJECT_ROOT/deploy</code> <a href="./prometheus-setup.html">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-overview-1"><a class="header" href="#system-overview-1">System Overview</a></h1>
<h2 id="entry-point"><a class="header" href="#entry-point">Entry Point</a></h2>
<p>The main entry point for running and managing experiments is <code>$PROJECT_ROOT/entrypoint.py</code>. This script provides CLI commands for setting up the environment, running experiments, and managing resources.</p>
<p>The entrypoint is meant to be edited and modified by the experimenter. There is some limited capability to pass in command-line arguments, however, for more complex usecases, it's meant to be easy to change.</p>
<h3 id="experiment-management"><a class="header" href="#experiment-management">Experiment Management</a></h3>
<p>Experiments are managed using the <code>Experiment</code> class in <code>$PROJECT_ROOT/greenflow/experiment.py</code>. This class handles the initialization, execution, and result calculation of experiments.</p>
<h3 id="configuration"><a class="header" href="#configuration">Configuration</a></h3>
<p>As stream processing systems have a large number of configuration parameters and tuneables, this project uses <a href="https://github.com/google/gin-config">gin-config</a> for configuration management. Refer <code>$PROJECT_ROOT/gin</code> for more.</p>
<p>Gin is a lightweight, Python-like configuration language that supports dependency injection. This allows us to define functions without explicitly passing each and every parameter that might change.</p>
<p>Configurations are defined in <code>.gin</code> files and can be dynamically re-bound using the <code>rebind_parameters</code> function in <code>entrypoint.py</code>.</p>
<h3 id="storage"><a class="header" href="#storage">Storage</a></h3>
<p>Experiment data is stored using TinyDB with custom serialization for handling complex data types. The <code>ExpStorage</code> class in <code>storage.py</code> manages the experiment database.</p>
<p>The experiment database is stored in plain-text in <code>yaml</code> format at <code>$PROJECT_ROOT/storage/experiment-history.yaml</code>. This includes any and all parameters, the Grid'5000 deployment information as well as the exact timestamps of <code>experiment_started_ts</code> and <code>deployment_started_ts</code> which are then used correspondingly as labels with the VictoriaMetrics/Prometheus TSDB to filter the results for further analysis. For more details, click <a href="./setup/prometheus-setup.html">here</a>.</p>
<h3 id="what-next"><a class="header" href="#what-next">What next?</a></h3>
<p>Now that you have a better idea of how the overall system works, <a href="./workflow.html">let's understand the overall workflow</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="workflow"><a class="header" href="#workflow">Workflow</a></h1>
<h2 id="entrypoint"><a class="header" href="#entrypoint">Entrypoint</a></h2>
<p>The <code>$PROJECT_ROOT/entrypoint.py</code> is meant to be readable and editable and customisable and provide an easy way to run various experiments and expose it as a command-line interface to be able to pass in parameters.</p>
<h2 id="workflow-steps"><a class="header" href="#workflow-steps">Workflow Steps</a></h2>
<h3 id="1-setting-up-the-environment"><a class="header" href="#1-setting-up-the-environment">1. Setting Up the Environment</a></h3>
<p><strong>Objective</strong>: Provision resources, deploy necessary services, and prepare the environment for experiments.</p>
<p><strong>Command</strong>:</p>
<pre><code class="language-sh">python entrypoint.py setup &lt;exp_name&gt; --workers &lt;num_workers&gt;
</code></pre>
<p>Details:</p>
<ul>
<li>Provision Resources: This step provisions the required resources (e.g., VMs or containers) and sets up the Kubernetes cluster if not already present.</li>
<li>Deploy Services: Deploys essential services like Prometheus, Scaphandre, Kafka, and Redpanda.</li>
<li>Warm-up: Initializes Kafka and Redpanda to ensure they are ready for experiments.
Note:</li>
<li>At its base, the G'5K provider is provided and is the main focus for the Greenflow project. However, provisioning is Customisable by implementing the <code>platform</code> interface (<code>$PROJECT_ROOT/greenflow/platform.py</code>)</li>
<li>If you already have a pre-existing set of nodes/VMs for the cluster, you can skip the provisioning step. Instead, run the <code>deploy_k3s</code> after setting up <code>$PROJECT_ROOT/ansible/inventory/hosts.yaml</code> in a similar format to the one below.</li>
</ul>
<pre><code class="language-yaml">all:
  children:
    broker:
      hosts:
        paravance-68.rennes.grid5000.fr:
          kubernetes_role: broker
        paravance-8.rennes.grid5000.fr:
          kubernetes_role: broker
        paravance-9.rennes.grid5000.fr:
          kubernetes_role: broker
    control:
      hosts:
        paravance-14.rennes.grid5000.fr:
          kubernetes_role: control_plane
    worker:
      hosts:
        paravance-16.rennes.grid5000.fr:
          kubernetes_role: node
        paravance-25.rennes.grid5000.fr:
          kubernetes_role: node
        paravance-32.rennes.grid5000.fr:
          kubernetes_role: node
</code></pre>
<h4 id="roles-in-the-cluster"><a class="header" href="#roles-in-the-cluster">Roles in the Cluster</a></h4>
<p>The <code>hosts.yaml</code> file defines different roles for the nodes in the cluster:</p>
<ol>
<li>
<p><strong>broker</strong>: These nodes are dedicated to running Kafka or Redpanda brokers. They handle message storage and distribution.</p>
</li>
<li>
<p><strong>control_plane</strong>: This node (or nodes) manages the Kubernetes cluster operations. It's responsible for maintaining the desired state of the cluster. It also serves as the metrics server, and all of the Kubernetes operators.</p>
</li>
<li>
<p><strong>worker</strong>: These nodes run the actual workloads (pods) in the Kubernetes cluster.</p>
</li>
</ol>
<p>This separation of roles allows for better resource allocation and performance optimization based on the specific requirements of each component.</p>
<h3 id="2-running-experiments"><a class="header" href="#2-running-experiments">2. Running Experiments</a></h3>
<p>Objective: Execute experiments with specified configurations to measure energy consumption.</p>
<pre><code>python entrypoint.py ingest &lt;exp_name&gt; --load &lt;load&gt; --message_size &lt;size&gt; --instances &lt;instances&gt; --partitions &lt;partitions&gt;
</code></pre>
<p>Details:
• Configuration: Load the appropriate configuration using load_gin.
• Execution: Run the experiment using the ingest command with specified parameters.
• Monitoring: The results are <a href="./setup/prometheus-setup.html">automatically collected</a> and written to the local (and <code>remote_write</code>) Prometheus/VictoriaMetrics server.
Objective: Terminate the current job and clean up resources.
Command:</p>
<p>python entrypoint.py killjob
Details:
• This step ensures that all resources are properly cleaned up after the experiment.
Interactive Shell
Objective: Provide a flexible environment for running setup steps and experiments without provisioning.
Command:</p>
<p>python entrypoint.py i &lt;exp_name&gt;
Details:
• Direct Access: Drop into an interactive shell with the current experiment configuration.
• Custom Setup: Run any setup steps manually, deploy specific services, or configure parameters as needed.
Customizing the Workflow
GreenFlow's workflow is highly customizable. Here are some key points to consider:
Pre-existing Kubernetes Cluster
• If you already have a Kubernetes cluster, you can skip the provisioning step in the setup.
• Use the interactive shell to deploy necessary services and configure your environment.
Deploying Theodolite
• Theodolite provides various use cases for stream processing systems.
• Deploy Theodolite using its playbook and configure it for specific use cases like UC1 Flink.
• For more details, refer to the Theodolite documentation.
Running Custom Workflows
• If you have a custom workflow, package your script into a Docker container and create a Kubernetes manifest.
• Use the provided playbooks to set up the necessary environment (e.g., Prometheus stack).
• Customize the experiment.yaml meta-template to define your experiment.
Example Workflow</p>
<pre><code></code></pre>
<h1 id="running-an-experiment"><a class="header" href="#running-an-experiment">Running an Experiment</a></h1>
<p>Currently the only workflow tested is to run an experiment specified in the meta-template <code>$PROJECT_ROOT/project/templates/exp.yaml.j2</code>. With a basic knowledge of Ansible, you can modify it and call the corresponding playbook from <code>entrypoint.py</code>.</p>
<pre><code>python entrypoint.py setup ingest-kafka --workers 3 #reserves nodes, sets up k3s, deploys all helm charts
python entrypoint.py ingest ingest-kafka # Runs the experiment specified under the ingest cli in entrypoint.py
</code></pre>
<h2 id="using-interactive-shell"><a class="header" href="#using-interactive-shell">Using Interactive Shell</a></h2>
<p>If you have a pre-existing Kubernetes cluster:</p>
<pre><code>python entrypoint.py i ingest-kafka
</code></pre>
<p>In the interactive shell, you can run setup steps manually:</p>
<pre><code class="language-sh"># Load configuration
load_gin(exp_name="ingest-kafka")
# Deploy Prometheus
p(prometheus)
# Deploy Kafka
p(kafka)
# Deploy Redpanda
p(redpanda)
# Deploy Theodolite
</code></pre>
<p>Conclusion
GreenFlow provides a flexible and customizable workflow for running energy consumption experiments. By understanding the key steps and leveraging the interactive shell, you can efficiently set up, run, and clean up your experiments. Customize the workflow to fit your specific needs and experiment configurations.</p>
<pre><code>
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prometheus-setup"><a class="header" href="#prometheus-setup">Prometheus Setup</a></h1>
<p>This document outlines the setup process for Prometheus within the GreenFlow project, explaining how it integrates with other components and the configuration required for successful deployment.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The Prometheus setup in GreenFlow is designed to collect metrics from experiments run on a Kubernetes cluster and store them in a local Docker Compose environment. This setup allows for flexible data collection and analysis agnostic to the network.</p>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<p>The approach taken in GreenFlow for setting up Prometheus and related components is driven by several key factors:</p>
<ol>
<li>
<p><strong>Network Agnosticism</strong>: Experiments may be run on various types of machines with different network configurations. This setup ensures consistency across different environments without requiring complex network setups.</p>
</li>
<li>
<p><strong>Ephemeral Experiment Environments</strong>: GreenFlow is designed to run on bare metal platforms with ephemeral nodes and reservations. No data is stored permanently on the cluster itself, necessitating a reliable external data collection solution.</p>
</li>
<li>
<p><strong>Simplified Setup</strong>: By leveraging Docker and Nix on the local machine, we ensure that experiments can be run without the need for a custom domain or a personally managed Prometheus server exposed to the web.</p>
</li>
<li>
<p><strong>Flexibility</strong>: This approach allows for data collection and analysis even when the experimental nodes are not on the same network as the analysis machine.</p>
</li>
<li>
<p><strong>Reproducibility</strong>: The consistent interface provided by this setup enhances the reproducibility of experiments across different configurations and environments.</p>
</li>
<li>
<p><strong>Data Persistence</strong>: While the experiment environment is ephemeral, this setup ensures that valuable metrics and data are safely stored and accessible for post-experiment analysis.</p>
</li>
</ol>
<p>By addressing these challenges, we provide a robust and reliable solution that simplifies the complexity of setting up experiments in diverse environments while ensuring data integrity and accessibility.</p>
<h2 id="key-components"><a class="header" href="#key-components">Key Components</a></h2>
<ol>
<li>Prometheus Helm Chart (deployed on Kubernetes, within the experimental cluster)</li>
<li>Local Docker Compose environment</li>
<li>Tailscale for secure networking</li>
</ol>
<h2 id="deployment-process"><a class="header" href="#deployment-process">Deployment Process</a></h2>
<h3 id="1-kubernetes-prometheus-setup"><a class="header" href="#1-kubernetes-prometheus-setup">1. Kubernetes Prometheus Setup</a></h3>
<p>The Prometheus stack is deployed on the Kubernetes cluster using a Helm chart. This Prometheus instance is configured to scale up before an experiment starts and scale down after it completes.</p>
<p>This configuration is part of the Ansible role <code>prometheus</code> and the values of the Helm chart are substituted in <code>$PROJECT_ROOT/ansible/project/roles/prometheus/templates/kube-prometheus-stack-values.yaml.j2</code>
Key configuration:</p>
<ul>
<li>Remote Write URL: This is set to point to the local Victoria Metrics instance.</li>
</ul>
<h3 id="2-local-docker-compose-environment"><a class="header" href="#2-local-docker-compose-environment">2. Local Docker Compose Environment</a></h3>
<p>The local environment runs several services:</p>
<ul>
<li>Victoria Metrics: Acts as the target Prometheus cluster, receiving data from the Kubernetes Prometheus instance.</li>
<li>Grafana: For visualizing the collected metrics.</li>
<li>Caddy: Reverse proxy for routing requests.</li>
<li>Tailscale: Provides secure networking between the Kubernetes cluster and local environment.</li>
</ul>
<h3 id="3-tailscale-configuration"><a class="header" href="#3-tailscale-configuration">3. Tailscale Configuration</a></h3>
<p>Tailscale is used to create a secure connection between the Kubernetes cluster and the local machine. This is especially useful when the local machine is behind NAT or firewalls.</p>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<p>Most of the user-specific configuration is captured in the <code>.secrets.env</code> file. This file is used by Ansible templates to populate the necessary values in the Prometheus role. After making any changes, make sure to run <code>direnv allow</code> or manually source the variables by running <code>source .secrets.env</code></p>
<p>Key configurations in <code>$PROJECT_ROOT/.secrets.env</code>.</p>
<h3 id="tailscale-setup"><a class="header" href="#tailscale-setup">Tailscale Setup</a></h3>
<ol>
<li>Set up a <a href="https://login.tailscale.com/admin/machines">Tailscale account</a>.</li>
<li>Generate a <a href="https://login.tailscale.com/admin/settings/keys">Tailscale auth key</a></li>
<li>Add the Tailscale auth key to the .env file in the Docker Compose directory <code>$PROJECT_ROOT/deploy</code></li>
</ol>
<h3 id="how-does-it-work"><a class="header" href="#how-does-it-work">How does it work?</a></h3>
<ol>
<li>Before an experiment:
The Prometheus stack in Kubernetes scales up.</li>
<li>During the experiment:
▪ Metrics are collected by the Kubernetes Prometheus instance.
▪ Data is also sent to the local Victoria Metrics instance via the <a href="https://prometheus.io/docs/specs/remote_write_spec/">Prometheus Remote Write API</a></li>
<li>After the experiment:
▪ The Kubernetes Prometheus stack scales down.
▪ Data remains in the local Victoria Metrics for analysis.</li>
</ol>
<h3 id="alternative-access-methods"><a class="header" href="#alternative-access-methods">Alternative Access Methods</a></h3>
<p>While Tailscale is recommended for its ease of use and security, alternative methods for accessing the Kubernetes cluster from the local machine include:</p>
<ul>
<li>ngrok</li>
<li>Cloudflare Tunnel</li>
<li>Direct access (if the local machine is accessible from all nodes of the Kubernetes cluster and firewall/ports are open)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-analysis"><a class="header" href="#data-analysis">Data Analysis</a></h1>
<p>This chapter provides an overview of how to analyze data collected from GreenFlow experiments. It includes a brief introduction to Prometheus querying with PromQL, and explains how to use the provided tools and scripts for data analysis.</p>
<h2 id="overview-of-prometheus-and-promql"><a class="header" href="#overview-of-prometheus-and-promql">Overview of Prometheus and PromQL</a></h2>
<p>Prometheus is a powerful monitoring and alerting toolkit that collects and stores metrics as time series data. PromQL (Prometheus Query Language) is used to query this data. Understanding the basics of PromQL is essential for effective data analysis.</p>
<h3 id="prometheus-metrics"><a class="header" href="#prometheus-metrics">Prometheus Metrics</a></h3>
<p>Prometheus metrics are stored with labels that provide additional context. A metric consists of:</p>
<ul>
<li><strong>Metric Name</strong>: Describes the type of data being measured (e.g., <code>http_requests_total</code>).</li>
<li><strong>Labels</strong>: Key-value pairs that provide additional information about the metric (e.g., <code>method="GET"</code>).</li>
</ul>
<h3 id="basic-promql-queries"><a class="header" href="#basic-promql-queries">Basic PromQL Queries</a></h3>
<ul>
<li><strong>Instant Vector</strong>: A set of time series containing a single sample for each time series, all sharing the same timestamp.</li>
</ul>
<pre><code class="language-promql">http_requests_total
</code></pre>
<ul>
<li><strong>Range Vector</strong>: A set of time series containing a range of data points over time.</li>
</ul>
<pre><code class="language-promql">http_requests_total[5m]
</code></pre>
<ul>
<li><strong>Aggregation</strong>: Summarizes or groups data.</li>
</ul>
<pre><code class="language-promql">sum(http_requests_total) by (method)
</code></pre>
<p>For more detailed information, refer to the <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Prometheus documentation</a>.</p>
<h2 id="data-analysis-in-greenflow"><a class="header" href="#data-analysis-in-greenflow">Data analysis in GreenFlow</a></h2>
<p>GreenFlow provides tools and scripts to facilitate data analysis. The primary tools include the analysis.py script and Jupyter notebooks.</p>
<p>Since the metrics are stored in the VictoriaMetrics instance, they can be queried and accessed through filtering for <code>experiment_started_ts</code> to zoom in on a particular experiment. Make use of Grafana for exploring and visualizing the data.</p>
<h3 id="analysis-script-analysispy"><a class="header" href="#analysis-script-analysispy">Analysis Script: analysis.py</a></h3>
<p>The analysis.py script contains helper functions for performing various calculations and data manipulations using pandas DataFrames. This script is used to automate the data analysis process.
Key functions in analysis.py:</p>
<ul>
<li>get_experiments(): Retrieves experiment data.</li>
<li>filter_experiments(): Filters experiments based on specified criteria.</li>
<li>enrich_dataframe(): Adds additional computed columns to the DataFrame for analysis.</li>
</ul>
<h2 id="example-workflow-in-jupyter-notebook"><a class="header" href="#example-workflow-in-jupyter-notebook">Example Workflow in Jupyter Notebook</a></h2>
<p>Below is an example workflow for analyzing data using a Jupyter notebook. This workflow includes steps for filtering experiments, enriching data, and creating visualizations.</p>
<pre><code class="language-python"># %%
from prometheus_api_client import PrometheusConnect, MetricRangeDataFrame
from prometheus_api_client.utils import parse_datetime
import pandas as pd
import pendulum
from greenflow.g import g
from tinydb import TinyDB, Query
from os import getenv
import matplotlib.pyplot as plt
import seaborn as sns
import greenflow.analysis as an
import qgridnext as qgrid

# %%
# Retrieve and filter experiments
cutoff = "2024-07-23T14:40:18.761822+02:00"
experiments = an.get_experiments()

def interest(exp) -&gt; bool:
  params = exp["experiment_metadata"]["factors"]["exp_params"]
  messageSize = params["messageSize"]
  partitions = params["partitions"]
  if "cluster=chirop" in exp["experiment_description"]:
    if messageSize &gt; 64 and partitions &gt;= 1:
      return True
  return False

redpanda_kafka_data = an.filter_experiments(experiments, interest, cutoff)

# %%
# Enrich data and visualize
enriched_data = an.enrich_dataframe(redpanda_kafka_data)
enriched_data = enriched_data[enriched_data["throughput_gap_percentage"] &lt; 0]

# Create a scatter plot
def create_graph(data, system_name):
    fig, ax = plt.subplots(figsize=(12, 8))
    cmap = plt.cm.RdYlGn
    scatter = ax.scatter(
        data["messageSize"], data["load"], c=data["throughput_gap_percentage"], cmap=cmap, s=50
    )
    plt.colorbar(scatter, ax=ax, label="Throughput Gap (%)", extend="min")
    ax.set_title(f"Load vs Message Size for {system_name}")
    ax.set_xlabel("Message Size (bytes)")
    ax.set_ylabel("Load (messages/sec)")
    plt.tight_layout()
    plt.show()

# Create graphs for Kafka and Redpanda
create_graph(enriched_data[enriched_data["exp_name"] == "ingest-kafka"], "Kafka")
create_graph(enriched_data[enriched_data["exp_name"] == "ingest-redpanda"], "Redpanda")
</code></pre>
<h2 id="custom-analysis"><a class="header" href="#custom-analysis">Custom Analysis</a></h2>
<p>For custom analysis, you can modify the provided Jupyter notebook or create your own analysis scripts using the functions in analysis.py. The Jupyter notebook provides an interactive environment for exploring and visualizing data, while the analysis.py script offers reusable functions for common analysis tasks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="project-info"><a class="header" href="#project-info">Project Info</a></h1>
<h2 id="greenflow"><a class="header" href="#greenflow">GreenFlow</a></h2>
<p>GreenFlow is an open-source research software, actively maintained at Inria. The project still has a lot of rough edges and gaps in the documentation and is currently in alpha. We appreciate your patience as we strive to improve its reliability.</p>
<h2 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h2>
<p>To get support with using GreenFlow, feel free to directly contact our team. You might also want to raise an issue on GitHub or GitLab.</p>
<h2 id="contributors"><a class="header" href="#contributors">Contributors</a></h2>
<ul>
<li><strong>Maintainer</strong>: Govind KP</li>
</ul>
<h2 id="contact"><a class="header" href="#contact">Contact</a></h2>
<p>For support/feedback/issues/questions, do not hesitate to contact me at <code>research &lt;at&gt; govind.work</code> or through:</p>
<ul>
<li><strong>GitHub</strong>: <a href="https://github.com/stream-energy/greenflow">GreenFlow</a></li>
</ul>
<p>We look forward to your contributions and feedback to help improve GreenFlow!</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
